### Data Format
```json
{
  "image_path": "The basename of image.",
  "task_type":"The category of the referring task ('attribute', 'positioin', 'interaction', 'relation', 'commonsense', 'reject').",
  "expression": "Referring expression.",
  "bbox":"The ground truth bounding box corresponding to the expression. For 'reject' tasks, this is an empty list [].",
  "width":"The original image width.",
  "height":"The original image height.",
}
```




## Benchmarking

To perform benchmark evaluation, you first need to run the model inference on the benchmark dataset and save its prediction results, **including phrase_type, bounding box** .
- `prediction_file` should be in `.jsonl` format, with each line formatted as follows: 
```json
{
  "image_path": "[Optional] The basename of image.",
  "task_type":"The category of the referring task ('attribute', 'positioin', 'interaction', 'relation', 'commonsense', 'reject').",
  "expression": "[Optional] Referring expression.",
  "bbox":"The ground truth bounding box corresponding to the expression. For 'reject' tasks, this is an empty list [].",
  "pred_bbox": "The predicted bounding box from the model, resized to match the original image's resolution.",
  "response":"[Optional] The full, raw text response generated by the model.",
}
```
**Then, calculate the metrics with :**
```sh
python get_prediction_Acc.py  --prediction_file your_prediction_file.jsonl
``` 

This example runs model inference and calculates benchmark accuracy in a single script.
```sh
python eval_qwen3vl.py
``` 
for multiprocessing.

For other models, you should modify the following parts:
- Model Loading and Inference: The code that loads your model and generates a response.
- extract_bbox_from_response(): To parse the bounding box from your model's raw output.
- resize_bbox(): To scale the bounding box coordinates to match the original image's resolution.

